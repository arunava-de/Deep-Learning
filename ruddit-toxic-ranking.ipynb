{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-16T14:52:37.798438Z","iopub.execute_input":"2021-12-16T14:52:37.799299Z","iopub.status.idle":"2021-12-16T14:52:37.834961Z","shell.execute_reply.started":"2021-12-16T14:52:37.799258Z","shell.execute_reply":"2021-12-16T14:52:37.832984Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import re\nimport time\nimport nltk\nfrom tqdm import tqdm\nimport lightgbm as lgb\ntqdm.pandas()\nfrom bs4 import BeautifulSoup\nimport matplotlib.pyplot as plt\n\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nlemmatizer = WordNetLemmatizer()\nstop = stopwords.words('english')","metadata":{"execution":{"iopub.status.busy":"2021-12-16T14:58:25.617834Z","iopub.execute_input":"2021-12-16T14:58:25.618283Z","iopub.status.idle":"2021-12-16T14:58:25.626039Z","shell.execute_reply.started":"2021-12-16T14:58:25.618242Z","shell.execute_reply":"2021-12-16T14:58:25.625054Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"ruddit_df = pd.read_csv(\"../input/ruddit-jigsaw-dataset/Dataset/ruddit_with_text.csv\")\ntoxic_classification_test = pd.read_csv(\"/kaggle/input/jigsaw-toxic-comment-classification-challenge/test.csv\")\ntoxic_classification_test_labels = pd.read_csv(\"/kaggle/input/jigsaw-toxic-comment-classification-challenge/test_labels.csv\")\ntoxic_classification_train = pd.read_csv(\"/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv\")\n\ndata_path = \"/kaggle/input/jigsaw-toxic-severity-rating/\"\nvalidation_data = pd.read_csv(data_path+\"validation_data.csv\")\nfinal_data = pd.read_csv(data_path+\"comments_to_score.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-12-16T14:52:39.261051Z","iopub.execute_input":"2021-12-16T14:52:39.261364Z","iopub.status.idle":"2021-12-16T14:52:43.629256Z","shell.execute_reply.started":"2021-12-16T14:52:39.261323Z","shell.execute_reply":"2021-12-16T14:52:43.628237Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Prepping the ruddit dataset","metadata":{}},{"cell_type":"code","source":"ruddit_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-16T14:52:43.630935Z","iopub.execute_input":"2021-12-16T14:52:43.631217Z","iopub.status.idle":"2021-12-16T14:52:43.651978Z","shell.execute_reply.started":"2021-12-16T14:52:43.631182Z","shell.execute_reply":"2021-12-16T14:52:43.651404Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"ruddit_df.shape","metadata":{"execution":{"iopub.status.busy":"2021-12-16T14:52:43.652999Z","iopub.execute_input":"2021-12-16T14:52:43.653869Z","iopub.status.idle":"2021-12-16T14:52:43.659083Z","shell.execute_reply.started":"2021-12-16T14:52:43.653833Z","shell.execute_reply":"2021-12-16T14:52:43.658481Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"### Looking at distribution of offensive scores","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots()\nruddit_df.offensiveness_score.plot(ax=ax, kind='hist', color='green', bins=50, alpha=0.7)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-16T14:52:43.660541Z","iopub.execute_input":"2021-12-16T14:52:43.660974Z","iopub.status.idle":"2021-12-16T14:52:43.995160Z","shell.execute_reply.started":"2021-12-16T14:52:43.660942Z","shell.execute_reply":"2021-12-16T14:52:43.994111Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### Writing a custom scoring function to convert offensiveness scores to toxicity scores","metadata":{}},{"cell_type":"code","source":"def custom_rank(score):\n    if score<=0:\n        return 0\n    else:\n        if score<0.2:\n            return 1\n        elif score>=0.2 and score<0.4:\n            return 2\n        elif score>=0.4 and score<0.7:\n            return 3\n        elif score>=0.7 and score<=1:\n            return 4\n\nruddit_df['toxicity'] = ruddit_df['offensiveness_score'].progress_apply(custom_rank)","metadata":{"execution":{"iopub.status.busy":"2021-12-16T14:52:45.806802Z","iopub.execute_input":"2021-12-16T14:52:45.807087Z","iopub.status.idle":"2021-12-16T14:52:45.828606Z","shell.execute_reply.started":"2021-12-16T14:52:45.807056Z","shell.execute_reply":"2021-12-16T14:52:45.827770Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"ruddit_df.columns.values","metadata":{"execution":{"iopub.status.busy":"2021-12-16T14:52:46.398028Z","iopub.execute_input":"2021-12-16T14:52:46.398360Z","iopub.status.idle":"2021-12-16T14:52:46.404184Z","shell.execute_reply.started":"2021-12-16T14:52:46.398320Z","shell.execute_reply":"2021-12-16T14:52:46.403369Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"del(ruddit_df['post_id'])\ndel(ruddit_df['comment_id'])\ndel(ruddit_df['url'])\ndel(ruddit_df['offensiveness_score'])","metadata":{"execution":{"iopub.status.busy":"2021-12-16T14:52:47.000063Z","iopub.execute_input":"2021-12-16T14:52:47.000764Z","iopub.status.idle":"2021-12-16T14:52:47.007089Z","shell.execute_reply.started":"2021-12-16T14:52:47.000718Z","shell.execute_reply":"2021-12-16T14:52:47.006195Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"ruddit_df.toxicity.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-12-16T14:52:48.000841Z","iopub.execute_input":"2021-12-16T14:52:48.001219Z","iopub.status.idle":"2021-12-16T14:52:48.010046Z","shell.execute_reply.started":"2021-12-16T14:52:48.001176Z","shell.execute_reply":"2021-12-16T14:52:48.009443Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"ruddit_df = ruddit_df.rename(columns={'txt':'comment_text'})","metadata":{"execution":{"iopub.status.busy":"2021-12-16T14:52:48.693935Z","iopub.execute_input":"2021-12-16T14:52:48.694293Z","iopub.status.idle":"2021-12-16T14:52:48.700210Z","shell.execute_reply.started":"2021-12-16T14:52:48.694260Z","shell.execute_reply":"2021-12-16T14:52:48.699185Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"df_test = pd.merge(toxic_classification_test, toxic_classification_test_labels, how='left', on='id')\ndf = pd.concat([toxic_classification_train, df_test])\n\ndf = df[(df['toxic']!=-1) & (df['severe_toxic']!=-1) & (df['obscene']!=-1) & (df['threat']!=-1) & (df['insult']!=-1) & (df['identity_hate']!=-1)]\ndf['toxicity'] = df[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].sum(axis=1)\ndf['toxicity'] = df['toxicity'].apply(lambda x: 4 if x in [4,5,6] else x)","metadata":{"execution":{"iopub.status.busy":"2021-12-16T14:52:49.281099Z","iopub.execute_input":"2021-12-16T14:52:49.281748Z","iopub.status.idle":"2021-12-16T14:52:49.568877Z","shell.execute_reply.started":"2021-12-16T14:52:49.281707Z","shell.execute_reply":"2021-12-16T14:52:49.567924Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"del(df['id'])\ndel(df['toxic'])\ndel(df['identity_hate'])\ndel(df['severe_toxic'])\ndel(df['obscene'])\ndel(df['threat'])\ndel(df['insult'])","metadata":{"execution":{"iopub.status.busy":"2021-12-16T14:52:49.684927Z","iopub.execute_input":"2021-12-16T14:52:49.685269Z","iopub.status.idle":"2021-12-16T14:52:49.693848Z","shell.execute_reply.started":"2021-12-16T14:52:49.685229Z","shell.execute_reply":"2021-12-16T14:52:49.692851Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"df = pd.concat([df, ruddit_df])","metadata":{"execution":{"iopub.status.busy":"2021-12-16T14:52:50.438689Z","iopub.execute_input":"2021-12-16T14:52:50.439531Z","iopub.status.idle":"2021-12-16T14:52:50.455542Z","shell.execute_reply.started":"2021-12-16T14:52:50.439479Z","shell.execute_reply":"2021-12-16T14:52:50.454627Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"df.toxicity.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-12-16T14:52:51.005316Z","iopub.execute_input":"2021-12-16T14:52:51.006033Z","iopub.status.idle":"2021-12-16T14:52:51.015454Z","shell.execute_reply.started":"2021-12-16T14:52:51.005990Z","shell.execute_reply":"2021-12-16T14:52:51.014214Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"## Cleaning the comments","metadata":{}},{"cell_type":"code","source":"def clean(data, col):\n    \n    data[col] = data[col].str.replace('https?://\\S+|www\\.\\S+', ' social medium ')      \n        \n    data[col] = data[col].str.lower()\n    data[col] = data[col].str.replace(\"4\", \"a\") \n    data[col] = data[col].str.replace(\"2\", \"l\")\n    data[col] = data[col].str.replace(\"5\", \"s\") \n    data[col] = data[col].str.replace(\"1\", \"i\") \n    data[col] = data[col].str.replace(\"!\", \"i\") \n    data[col] = data[col].str.replace(\"|\", \"i\") \n    data[col] = data[col].str.replace(\"0\", \"o\") \n    data[col] = data[col].str.replace(\"l3\", \"b\") \n    data[col] = data[col].str.replace(\"7\", \"t\") \n    data[col] = data[col].str.replace(\"7\", \"+\") \n    data[col] = data[col].str.replace(\"8\", \"ate\") \n    data[col] = data[col].str.replace(\"3\", \"e\") \n    data[col] = data[col].str.replace(\"9\", \"g\")\n    data[col] = data[col].str.replace(\"6\", \"g\")\n    data[col] = data[col].str.replace(\"@\", \"a\")\n    data[col] = data[col].str.replace(\"$\", \"s\")\n    data[col] = data[col].str.replace(\"#ofc\", \" of fuckin course \")\n    data[col] = data[col].str.replace(\"fggt\", \" faggot \")\n    data[col] = data[col].str.replace(\"your\", \" your \")\n    data[col] = data[col].str.replace(\"self\", \" self \")\n    data[col] = data[col].str.replace(\"cuntbag\", \" cunt bag \")\n    data[col] = data[col].str.replace(\"fartchina\", \" fart china \")    \n    data[col] = data[col].str.replace(\"youi\", \" you i \")\n    data[col] = data[col].str.replace(\"cunti\", \" cunt i \")\n    data[col] = data[col].str.replace(\"sucki\", \" suck i \")\n    data[col] = data[col].str.replace(\"pagedelete\", \" page delete \")\n    data[col] = data[col].str.replace(\"cuntsi\", \" cuntsi \")\n    data[col] = data[col].str.replace(\"i'm\", \" i am \")\n    data[col] = data[col].str.replace(\"offuck\", \" of fuck \")\n    data[col] = data[col].str.replace(\"centraliststupid\", \" central ist stupid \")\n    data[col] = data[col].str.replace(\"hitleri\", \" hitler i \")\n    data[col] = data[col].str.replace(\"i've\", \" i have \")\n    data[col] = data[col].str.replace(\"i'll\", \" sick \")\n    data[col] = data[col].str.replace(\"fuck\", \" fuck \")\n    data[col] = data[col].str.replace(\"f u c k\", \" fuck \")\n    data[col] = data[col].str.replace(\"shit\", \" shit \")\n    data[col] = data[col].str.replace(\"bunksteve\", \" bunk steve \")\n    data[col] = data[col].str.replace('wikipedia', ' social medium ')\n    data[col] = data[col].str.replace(\"faggot\", \" faggot \")\n    data[col] = data[col].str.replace(\"delanoy\", \" delanoy \")\n    data[col] = data[col].str.replace(\"jewish\", \" jewish \")\n    data[col] = data[col].str.replace(\"sexsex\", \" sex \")\n    data[col] = data[col].str.replace(\"allii\", \" all ii \")\n    data[col] = data[col].str.replace(\"i'd\", \" i had \")\n    data[col] = data[col].str.replace(\"'s\", \" is \")\n    data[col] = data[col].str.replace(\"youbollocks\", \" you bollocks \")\n    data[col] = data[col].str.replace(\"dick\", \" dick \")\n    data[col] = data[col].str.replace(\"cuntsi\", \" cuntsi \")\n    data[col] = data[col].str.replace(\"mothjer\", \" mother \")\n    data[col] = data[col].str.replace(\"cuntfranks\", \" cunt \")\n    data[col] = data[col].str.replace(\"ullmann\", \" jewish \")\n    data[col] = data[col].str.replace(\"mr.\", \" mister \")\n    data[col] = data[col].str.replace(\"aidsaids\", \" aids \")\n    data[col] = data[col].str.replace(\"njgw\", \" nigger \")\n    data[col] = data[col].str.replace(\"wiki\", \" social medium \")\n    data[col] = data[col].str.replace(\"administrator\", \" admin \")\n    data[col] = data[col].str.replace(\"gamaliel\", \" jewish \")\n    data[col] = data[col].str.replace(\"rvv\", \" vanadalism \")\n    data[col] = data[col].str.replace(\"admins\", \" admin \")\n    data[col] = data[col].str.replace(\"pensnsnniensnsn\", \" penis \")\n    data[col] = data[col].str.replace(\"pneis\", \" penis \")\n    data[col] = data[col].str.replace(\"pennnis\", \" penis \")\n    data[col] = data[col].str.replace(\"pov.\", \" point of view \")\n    data[col] = data[col].str.replace(\"vandalising\", \" vandalism \")\n    data[col] = data[col].str.replace(\"cock\", \" dick \")\n    data[col] = data[col].str.replace(\"asshole\", \" asshole \")\n    data[col] = data[col].str.replace(\"youi\", \" you \")\n    data[col] = data[col].str.replace(\"afd\", \" all fucking day \")\n    data[col] = data[col].str.replace(\"sockpuppets\", \" sockpuppetry \")\n    data[col] = data[col].str.replace(\"iiprick\", \" iprick \")\n    data[col] = data[col].str.replace(\"penisi\", \" penis \")\n    data[col] = data[col].str.replace(\"warrior\", \" warrior \")\n    data[col] = data[col].str.replace(\"loil\", \" laughing out insanely loud \")\n    data[col] = data[col].str.replace(\"vandalise\", \" vanadalism \")\n    data[col] = data[col].str.replace(\"helli\", \" helli \")\n    data[col] = data[col].str.replace(\"lunchablesi\", \" lunchablesi \")\n    data[col] = data[col].str.replace(\"special\", \" special \")\n    data[col] = data[col].str.replace(\"ilol\", \" i lol \")\n    data[col] = data[col].str.replace(r'\\b[uU]\\b', 'you')\n    data[col] = data[col].str.replace(r\"what's\", \"what is \")\n    data[col] = data[col].str.replace(r\"\\'s\", \" is \")\n    data[col] = data[col].str.replace(r\"\\'ve\", \" have \")\n    data[col] = data[col].str.replace(r\"can't\", \"cannot \")\n    data[col] = data[col].str.replace(r\"n't\", \" not \")\n    data[col] = data[col].str.replace(r\"i'm\", \"i am \")\n    data[col] = data[col].str.replace(r\"\\'re\", \" are \")\n    data[col] = data[col].str.replace(r\"\\'d\", \" would \")\n    data[col] = data[col].str.replace(r\"\\'ll\", \" will \")\n    data[col] = data[col].str.replace(r\"\\'scuse\", \" excuse \")\n    data[col] = data[col].str.replace('\\s+', ' ')  # will remove more than one whitespace character\n#     text = re.sub(r'\\b([^\\W\\d_]+)(\\s+\\1)+\\b', r'\\1', re.sub(r'\\W+', ' ', text).strip(), flags=re.I)  # remove repeating words coming immediately one after another\n    data[col] = data[col].str.replace(r'(.)\\1+', r'\\1\\1') # 2 or more characters are replaced by 2 characters\n#     text = re.sub(r'((\\b\\w+\\b.{1,2}\\w+\\b)+).+\\1', r'\\1', text, flags = re.I)\n    data[col] = data[col].str.replace(\"[:|♣|'|§|♠|*|/|?|=|%|&|-|#|•|~|^|>|<|►|_]\", '')\n    \n    \n    data[col] = data[col].str.replace(r\"what's\", \"what is \")    \n    data[col] = data[col].str.replace(r\"\\'ve\", \" have \")\n    data[col] = data[col].str.replace(r\"can't\", \"cannot \")\n    data[col] = data[col].str.replace(r\"n't\", \" not \")\n    data[col] = data[col].str.replace(r\"i'm\", \"i am \")\n    data[col] = data[col].str.replace(r\"\\'re\", \" are \")\n    data[col] = data[col].str.replace(r\"\\'d\", \" would \")\n    data[col] = data[col].str.replace(r\"\\'ll\", \" will \")\n    data[col] = data[col].str.replace(r\"\\'scuse\", \" excuse \")\n    data[col] = data[col].str.replace(r\"\\'s\", \" \")\n\n    # Clean some punctutations\n    data[col] = data[col].str.replace('\\n', ' \\n ')\n    data[col] = data[col].str.replace(r'([a-zA-Z]+)([/!?.])([a-zA-Z]+)',r'\\1 \\2 \\3')\n    # Replace repeating characters more than 3 times to length of 3\n    data[col] = data[col].str.replace(r'([*!?\\'])\\1\\1{2,}',r'\\1\\1\\1')    \n    # Add space around repeating characters\n    data[col] = data[col].str.replace(r'([*!?\\']+)',r' \\1 ')    \n    # patterns with repeating characters \n    data[col] = data[col].str.replace(r'([a-zA-Z])\\1{2,}\\b',r'\\1\\1')\n    data[col] = data[col].str.replace(r'([a-zA-Z])\\1\\1{2,}\\B',r'\\1\\1\\1')\n    data[col] = data[col].str.replace(r'[ ]{2,}',' ').str.strip()   \n    data[col] = data[col].str.replace(r'[ ]{2,}',' ').str.strip()   \n    data[col] = data[col].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n    \n    return data\n\ndef text_cleaning(text):\n    '''\n    Cleans text into a basic form for NLP. Operations include the following:-\n    1. Remove special charecters like &, #, etc\n    2. Removes extra spaces\n    3. Removes embedded URL links\n    4. Removes HTML tags\n    5. Removes emojis\n    \n    text - Text piece to be cleaned.\n    '''\n    template = re.compile(r'https?://\\S+|www\\.\\S+')  # Removes website links\n    text = template.sub(r'', text)\n    \n    soup = BeautifulSoup(text, 'lxml')  # Removes HTML tags\n    only_text = soup.get_text()\n    text = only_text\n    \n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    \n    text = re.sub(r\"[^a-zA-Z\\d]\", \" \", text) # Remove special Charecters\n    text = re.sub(' +', ' ', text) # Remove Extra Spaces\n    text = text.strip().lower() # remove spaces at the beginning and at the end of string and make string lower\n    \n    # lemmatization\n    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split(' ')])\n    # del stopwords\n    text = ' '.join([word for word in text.split(' ') if word not in stop])\n\n    return text","metadata":{"execution":{"iopub.status.busy":"2021-12-16T14:52:54.009703Z","iopub.execute_input":"2021-12-16T14:52:54.010331Z","iopub.status.idle":"2021-12-16T14:52:54.055783Z","shell.execute_reply.started":"2021-12-16T14:52:54.010292Z","shell.execute_reply":"2021-12-16T14:52:54.055070Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"sampled_df = df.groupby('toxicity', group_keys=False).apply(lambda x: x.sample(min(len(x), 9000)))","metadata":{"execution":{"iopub.status.busy":"2021-12-16T14:52:54.295280Z","iopub.execute_input":"2021-12-16T14:52:54.295606Z","iopub.status.idle":"2021-12-16T14:52:54.339935Z","shell.execute_reply.started":"2021-12-16T14:52:54.295571Z","shell.execute_reply":"2021-12-16T14:52:54.339302Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"sampled_df = clean(sampled_df, 'comment_text')\nsampled_df['clean_text'] = sampled_df['comment_text'].progress_apply(text_cleaning)","metadata":{"execution":{"iopub.status.busy":"2021-12-16T14:52:56.124837Z","iopub.execute_input":"2021-12-16T14:52:56.125680Z","iopub.status.idle":"2021-12-16T14:53:29.511300Z","shell.execute_reply.started":"2021-12-16T14:52:56.125638Z","shell.execute_reply":"2021-12-16T14:53:29.509879Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"## Fitting a TF-IDF + lightGBM model","metadata":{"execution":{"iopub.status.busy":"2021-12-16T14:33:51.191537Z","iopub.execute_input":"2021-12-16T14:33:51.191794Z","iopub.status.idle":"2021-12-16T14:33:51.202659Z","shell.execute_reply.started":"2021-12-16T14:33:51.191766Z","shell.execute_reply":"2021-12-16T14:33:51.201932Z"}}},{"cell_type":"code","source":"vectorizer = TfidfVectorizer(min_df=3, max_df=0.5, analyzer='char_wb', ngram_range=(3,5))\ncomments_tr = vectorizer.fit_transform(sampled_df.clean_text)\ncomments_tr","metadata":{"execution":{"iopub.status.busy":"2021-12-16T14:55:01.096649Z","iopub.execute_input":"2021-12-16T14:55:01.097501Z","iopub.status.idle":"2021-12-16T14:55:11.208954Z","shell.execute_reply.started":"2021-12-16T14:55:01.097462Z","shell.execute_reply":"2021-12-16T14:55:11.207951Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"### Doing SVD to bring down the number of features","metadata":{}},{"cell_type":"code","source":"\n# comments_tr = StandardScaler(with_mean=False).fit_transform(comments_tr)\n\n# pca = PCA(n_components=10000)\n# principalComponents = pca.fit_transform(comments_tr)\n\n# svd = TruncatedSVD(n_components=10000, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# X_train, X_dev, y_train, y_dev = train_test_split(comments_tr.toarray(), sampled_df.toxicity, test_size=0.25, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2021-12-16T14:50:36.185329Z","iopub.execute_input":"2021-12-16T14:50:36.185644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start = time.time()\nmodel = lgb.LGBMClassifier(learning_rate=0.05,random_state=42)\nmodel.fit(comments_tr,sampled_df.toxicity,\n          verbose=20,eval_metric='logloss')\n\nend = time.time()\n\nprint(\"Time taken:\", end-start)","metadata":{"execution":{"iopub.status.busy":"2021-12-16T15:01:40.823769Z","iopub.execute_input":"2021-12-16T15:01:40.824639Z","iopub.status.idle":"2021-12-16T15:08:12.203699Z","shell.execute_reply.started":"2021-12-16T15:01:40.824581Z","shell.execute_reply":"2021-12-16T15:08:12.202759Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"def get_rank_score(vecs):\n    labels = model.predict(vecs)\n    prob_scores = model.predict_proba(vecs)\n    rank_scores = labels + np.max(prob_scores, axis=1)\n    return rank_scores\n\nless_toxic_vecs = vectorizer.transform(validation_data.less_toxic)\nmore_toxic_vecs = vectorizer.transform(validation_data.more_toxic)\n\nless_toxic_score = get_rank_score(less_toxic_vecs)\nmore_toxic_score = get_rank_score(more_toxic_vecs)","metadata":{"execution":{"iopub.status.busy":"2021-12-16T15:08:27.824959Z","iopub.execute_input":"2021-12-16T15:08:27.826046Z","iopub.status.idle":"2021-12-16T15:09:06.998356Z","shell.execute_reply.started":"2021-12-16T15:08:27.825992Z","shell.execute_reply":"2021-12-16T15:09:06.997379Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"np.sum(less_toxic_score<more_toxic_score)*100/validation_data.shape[0]","metadata":{"execution":{"iopub.status.busy":"2021-12-16T15:09:28.143365Z","iopub.execute_input":"2021-12-16T15:09:28.143889Z","iopub.status.idle":"2021-12-16T15:09:28.149709Z","shell.execute_reply.started":"2021-12-16T15:09:28.143853Z","shell.execute_reply":"2021-12-16T15:09:28.148914Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}