import pandas as pd
import numpy as np
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix

path = "./movie-review-sentiment-analysis-kernels-only/"
train = pd.read_csv(path+"train.tsv",sep='\t')
#First let us try a few normal classification models after removing stopwords and getting feature vector
phrases = list(train['Phrase'].copy())
stop_words = set(stopwords.words('english')) 
for i in range(train.shape[0]):
    words = word_tokenize(phrases[i])
    filtered = ""
    for w in words:
        if w not in stop_words:
            filtered = filtered + " " + w
    phrases[i] = filtered[1:]
    
vectorizer = TfidfVectorizer(max_features = 1000)
features = vectorizer.fit_transform(phrases).toarray()
labels = train['Sentiment'].copy()

X_train, X_test, Y_train, Y_test = train_test_split(features, labels, test_size=0.1, random_state=42)

logistic_clf = LogisticRegression().fit(X_train, Y_train)
Y_pred = logistic_clf.predict(X_test)
M = confusion_matrix(Y_test, Y_pred)
acc = np.trace(M)/np.sum(M)
print(acc)

0.577918749199026
#Accuracy quite bad when using just tf idf vectorizer for single words. Let's try bigrams and trigrams instead

vectorizer1 = TfidfVectorizer(max_features = 5000,ngram_range=(2,3))
features1 = vectorizer1.fit_transform(phrases).toarray()
X_train, X_test, Y_train, Y_test = train_test_split(features1, labels, test_size=0.1, random_state=22)

logistic_clf = LogisticRegression().fit(X_train, Y_train)
Y_pred = logistic_clf.predict(X_test)
M = confusion_matrix(Y_test, Y_pred)
acc = np.trace(M)/np.sum(M)
print(acc)
0.5376137383057799

#Worse than before! Quite surprising, even though we took more features
#Let us try random forests and XGB before moving on to word embeddings

X_train, X_test, Y_train, Y_test = train_test_split(features, labels, test_size=0.1, random_state=22)

from sklearn.ensemble import RandomForestClassifier
rf_clf = RandomForestClassifier(n_estimators=500, max_depth=5, random_state=0).fit(X_train, Y_train)

Y_pred = rf_clf.predict(X_test)
M = confusion_matrix(Y_test, Y_pred)
acc = np.trace(M)/np.sum(M)
print(acc)

0.5053184672561836

#Again quite bad. Seems like will need to use RNNs after all
