{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detailed-enlargement",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import umap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interior-graham",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./data/jigsaw-toxic-severity-rating/\"\n",
    "validation_data = pd.read_csv(data_path+\"validation_data.csv\")\n",
    "data = pd.read_csv(data_path+\"comments_to_score.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "missing-reputation",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "variable-orbit",
   "metadata": {},
   "outputs": [],
   "source": [
    "worker_freq_dict = dict(validation_data.worker.value_counts())\n",
    "workers = sorted(list(worker_freq_dict.keys()))\n",
    "worked = [worker_freq_dict[w] for w in workers]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "induced-guess",
   "metadata": {},
   "source": [
    "### Plotting worker productivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daily-forge",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(workers, worked, align='center', color='green')\n",
    "# plt.xticks(workers, worked)\n",
    "plt.title(\"Work distribution\")\n",
    "plt.xlabel(\"Worker ID\")\n",
    "plt.ylabel(\"Data points annotated\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designed-celebrity",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "validation_data.worker.value_counts().plot(ax=ax, kind='hist', color='green')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overall-escape",
   "metadata": {},
   "source": [
    "### Plotting length of each comment against type of comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demographic-mouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "less_toxic_lens = validation_data.less_toxic.apply(lambda x: len(x.split()))\n",
    "more_toxic_lens = validation_data.more_toxic.apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considered-syntax",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize =(10, 6))\n",
    " \n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "\n",
    "bp = ax.boxplot([less_toxic_lens, more_toxic_lens])\n",
    "\n",
    "ax.set_xticklabels(['less toxic', 'more toxic'])\n",
    "\n",
    "plt.title(\"Comment lengths box plot\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scientific-klein",
   "metadata": {},
   "source": [
    "## Plotting few vectors in 2D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beginning-requirement",
   "metadata": {},
   "source": [
    "Here we look at various embedding approaches and use UMAP to project them to 2D space and plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collective-jones",
   "metadata": {},
   "source": [
    "### Tf-idf plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satellite-attachment",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=500)\n",
    "corpus = np.concatenate([validation_data.less_toxic.apply(lambda x: x.replace(\"\\n\", \"\")).values, \n",
    "                         validation_data.more_toxic.apply(lambda x: x.replace(\"\\n\", \"\")).values])\n",
    "tfidf_vecs = vectorizer.fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "static-possible",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df = validation_data.sample(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electrical-poetry",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_corpus_less = sampled_df.less_toxic.apply(lambda x: x.replace(\"\\n\", \"\")).values\n",
    "\n",
    "sampled_corpus_more = sampled_df.more_toxic.apply(lambda x: x.replace(\"\\n\", \"\")).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continent-photography",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_corpus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viral-store",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vecs_less = vectorizer.transform(sampled_corpus_less).toarray()\n",
    "tfidf_vecs_more = vectorizer.transform(sampled_corpus_more).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personal-spanking",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = umap.UMAP(n_components=2,\n",
    "                      n_neighbors=15,\n",
    "                      min_dist=0.3,\n",
    "                      metric='correlation').fit_transform(np.concatenate([tfidf_vecs_less, tfidf_vecs_more]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "urban-damages",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_less = embedding[:5000,0]\n",
    "y_less = embedding[:5000,1]\n",
    "x_more = embedding[5000:,0]\n",
    "y_more = embedding[5000:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competent-bronze",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.scatter(x_less, y_less, color='green', alpha=0.5, label='less toxic')\n",
    "ax1.scatter(x_more, y_more, color='red', alpha=0.5, label='more toxic')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vietnamese-reading",
   "metadata": {},
   "source": [
    "We can see that both overlap quite a bit. We might need to move over to better embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nuclear-roman",
   "metadata": {},
   "source": [
    "### BERT plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intelligent-rolling",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blank-wrapping",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased',\n",
    "                                  output_hidden_states = True, # Whether the model returns all hidden-states.\n",
    "                                  )\n",
    "\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stone-optimum",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embedding(text):\n",
    "    \n",
    "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "\n",
    "    # Split the sentence into tokens.\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "    \n",
    "    # Truncating to 512 tokens\n",
    "    tokenized_text = tokenized_text[:512]\n",
    "\n",
    "    # Map the token strings to their vocabulary indeces.\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    segments_ids = [1] * len(tokenized_text)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        outputs = model(tokens_tensor, segments_tensors)\n",
    "\n",
    "        # Evaluating the model will return a different number of objects based on \n",
    "        # how it's  configured in the `from_pretrained` call earlier. In this case, \n",
    "        # becase we set `output_hidden_states = True`, the third item will be the \n",
    "        # hidden states from all layers. See the documentation for more details:\n",
    "        # https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
    "        hidden_states = outputs[2]\n",
    "        \n",
    "#     token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "#     token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "#     token_embeddings = token_embeddings.permute(1,0,2)\n",
    "    token_vecs = hidden_states[-2][0]\n",
    "    sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "    \n",
    "    return sentence_embedding.cpu().detach().numpy()\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complimentary-addition",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    text = text.replace(\"\\n\", \"\")\n",
    "    text = \" \".join(text.split())\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'[0-9]', '', text)\n",
    "    return text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logical-inspiration",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df = validation_data.sample(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "veterinary-caribbean",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df['less_toxic_cleaned'] = sampled_df['less_toxic'].progress_apply(clean)\n",
    "sampled_df['more_toxic_cleaned'] = sampled_df['more_toxic'].progress_apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spectacular-publisher",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df['less_toxic_vecs'] = sampled_df['less_toxic_cleaned'].progress_apply(get_bert_embedding)\n",
    "sampled_df['more_toxic_vecs'] = sampled_df['more_toxic_cleaned'].progress_apply(get_bert_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dried-silence",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(sampled_df.less_toxic_vecs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharing-consumer",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = umap.UMAP(n_components=2,\n",
    "                      n_neighbors=15,\n",
    "                      min_dist=0.3,\n",
    "                      metric='correlation').fit_transform(np.concatenate([list(sampled_df.less_toxic_vecs), \n",
    "                                                                          list(sampled_df.more_toxic_vecs)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaptive-female",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_less = embedding[:500,0]\n",
    "y_less = embedding[:500,1]\n",
    "x_more = embedding[500:,0]\n",
    "y_more = embedding[500:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "front-findings",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.scatter(x_less, y_less, color='cyan', alpha=0.5, label='less toxic')\n",
    "ax1.scatter(x_more, y_more, color='pink', alpha=0.5, label='more toxic')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ready-austin",
   "metadata": {},
   "source": [
    "Too much overlap of BERT vectors as well. Seems unlikely that a classifier can do much"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experienced-evans",
   "metadata": {},
   "source": [
    "## Fitting a baseline SVM + BERT model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romantic-classroom",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
